{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os \n",
    "import sys\n",
    "sys.path.append('./../../.')\n",
    "sys.path.append('./../.')\n",
    "import glob \n",
    "from tqdm import tqdm \n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()\n",
    "import re\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "DATA_SOURCE = './../generated_data_v1/'\n",
    "CONFIG = None\n",
    "DIR_LOC = None\n",
    "CONFIG = None\n",
    "CONFIG_FILE = 'config.yaml'\n",
    "id_col = 'PanjivaRecordID'\n",
    "non_CoOcc_dict = {}\n",
    "numeric_col_stats = {}\n",
    "DATA_PATH = ''\n",
    "\n",
    "def set_up_config(_DIR = None):\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global CONFIG_FILE\n",
    "    global use_cols\n",
    "    global freq_bound\n",
    "    global num_neg_samples_ape\n",
    "    global save_dir\n",
    "    global column_value_filters\n",
    "    global num_neg_samples\n",
    "    global NUMERIC_COLUMNS\n",
    "    global id_col\n",
    "    global DISCRETE_COLUMNS\n",
    "    global DATA_PATH\n",
    "    global DATA_SOURCE\n",
    "    \n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "    \n",
    "    DATA_PATH = os.path.join(DATA_SOURCE, DIR)\n",
    "    if _DIR is not None:\n",
    "        DIR = _DIR\n",
    "        CONFIG['DIR'] = _DIR\n",
    "    else:\n",
    "        DIR = CONFIG['DIR']\n",
    "\n",
    "    DIR_LOC = re.sub('[0-9]', '', DIR)\n",
    "    DATA_PATH = os.path.join(DATA_SOURCE, DIR)\n",
    "    save_dir =  CONFIG['save_dir']\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    save_dir = os.path.join(\n",
    "        CONFIG['save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "        \n",
    "    use_cols = CONFIG[DIR]['use_cols']\n",
    " \n",
    "    NUMERIC_COLUMNS = CONFIG[DIR]['numeric_columns']\n",
    "    _cols = list(use_cols)\n",
    "    _cols.remove(id_col)\n",
    "    for nc in NUMERIC_COLUMNS:\n",
    "        _cols.remove(nc)\n",
    "        \n",
    "    DISCRETE_COLUMNS = list(sorted(_cols))\n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "def fetch_data_sets():\n",
    "    global DATA_PATH\n",
    "    df_train = pd.read_csv(os.path.join(DATA_PATH,'train_data.csv'),index_col=None)\n",
    "    df_test = pd.read_csv(os.path.join(DATA_PATH,'test_data.csv'),index_col=None)\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def hash1(row,c1,c2):\n",
    "    return str(row[c1]) + '_' + str(row[c2])\n",
    "\n",
    "\n",
    "def create_coocc_dict(df_train):\n",
    "    global DISCRETE_COLUMNS\n",
    "    global non_CoOcc_dict\n",
    "    df = df_train.copy()\n",
    "    for pair in combinations(DISCRETE_COLUMNS,2):\n",
    "        pool = []\n",
    "        col_1 = pair[0]\n",
    "        col_2 = pair[1]\n",
    "        tmp = df.groupby([col_1,col_2]).size().reset_index(name='count')\n",
    "        \n",
    "        a = set(df[col_1])\n",
    "        b = set(df[col_2])\n",
    "        tmp['_hash'] = tmp.parallel_apply(hash1, axis=1, args=(col_1,col_2,))\n",
    "        present = list(tmp['_hash'])\n",
    "        for i, j in itertools.product(a, b):\n",
    "            pool.append(str(i) + '_' + str(j))\n",
    "        \n",
    "        pool = set(pool)\n",
    "     \n",
    "        pool = pool.difference(present)\n",
    "        \n",
    "        pool = [ _.split('_') for _ in pool ]\n",
    "        key = col_1 + '_' + col_2\n",
    "        print(len(pool))\n",
    "        non_CoOcc_dict[key] = pool\n",
    "    return \n",
    "\n",
    "def perturb_row(row, num_pairs = 1, num_numeric_cols=1 ):\n",
    "    global DISCRETE_COLUMNS\n",
    "    global NUMERIC_COLUMNS\n",
    "    global numeric_col_stats \n",
    "    global non_CoOcc_dict\n",
    "    row = row.copy()\n",
    "    pert_cols = np.random.choice( DISCRETE_COLUMNS , size= num_pairs*2, replace = False)\n",
    "    for i in range(1, num_pairs+1):\n",
    "        cols = sorted(pert_cols[(i-1)*2:(i-1)*2+2])\n",
    "        cols = sorted(cols)\n",
    "        col_1 = cols[0]\n",
    "        col_2 = cols[1]\n",
    "        key = col_1 + '_' + col_2\n",
    "        arr =  non_CoOcc_dict[key]\n",
    "        _idx = random.randint(0, len(arr))\n",
    "        vals = arr[_idx]\n",
    "        row[col_1] = vals[0]\n",
    "        row[col_2] = vals[1]\n",
    "    \n",
    "    # Select a numeric column \n",
    "    \n",
    "    numeric_cols = np.random.choice( NUMERIC_COLUMNS , size = num_numeric_cols, replace = False)\n",
    "    for nc in numeric_cols:\n",
    "        val = row[nc]\n",
    "        if val < 0.5:\n",
    "            val += random.uniform(0.25,0.75) \n",
    "        else:\n",
    "            val -= random.uniform(0.25,0.75) \n",
    "        row[nc] = val\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================================================================\n",
    "# Main code \n",
    "# ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2118671\n",
      "772048\n",
      "110491\n",
      "18456\n",
      "58576\n",
      "56038\n",
      "2220610\n",
      "5120156\n",
      "733139\n",
      "128272\n",
      "399489\n",
      "374255\n",
      "14608460\n",
      "264632\n",
      "43218\n",
      "141399\n",
      "134051\n",
      "5369725\n",
      "6300\n",
      "19805\n",
      "19172\n",
      "770584\n",
      "3686\n",
      "2690\n",
      "133000\n",
      "9415\n",
      "416646\n",
      "395415\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ddatta/anaconda3/envs/SG/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/ddatta/anaconda3/envs/SG/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 59, in global_worker\n    return _func(x)\n  File \"/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 111, in wrapper\n    **kwargs\n  File \"/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/pandarallel/data_types/dataframe.py\", line 31, in worker\n    return df.apply(func, *args, **kwargs)\n  File \"/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/pandas/core/frame.py\", line 6878, in apply\n    return op.get_result()\n  File \"/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/pandas/core/apply.py\", line 186, in get_result\n    return self.apply_standard()\n  File \"/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/pandas/core/apply.py\", line 313, in apply_standard\n    results, res_index = self.apply_series_generator()\n  File \"/home/ddatta/anaconda3/envs/SG/lib/python3.7/site-packages/pandas/core/apply.py\", line 341, in apply_series_generator\n    results[i] = self.f(v)\n  File \"<ipython-input-1-5c637b8cc88c>\", line 137, in perturb_row\n    vals = arr[_idx]\nIndexError: list index out of range\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ee363ce5322f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m df_3 = df_1.parallel_apply(\n\u001b[1;32m     28\u001b[0m     \u001b[0mperturb_row\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Save anomalies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SG/lib/python3.7/site-packages/pandarallel/pandarallel.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(data, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0minput_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0moutput_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                 \u001b[0mmap_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             )\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SG/lib/python3.7/site-packages/pandarallel/pandarallel.py\u001b[0m in \u001b[0;36mget_workers_result\u001b[0;34m(use_memory_fs, nb_workers, show_progress_bar, nb_columns, queue, chunk_lengths, input_files, output_files, map_result)\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0mprogress_bars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogresses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     return (\n",
      "\u001b[0;32m~/anaconda3/envs/SG/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "DIR = 'us_import3'\n",
    "set_up_config(DIR)\n",
    "\n",
    "# Normalise the numeric values\n",
    "def normalize(val,_min,_max):\n",
    "    return (val-_min)/(_max - _min)\n",
    "\n",
    "df_train, df_test = fetch_data_sets()\n",
    "for nc in NUMERIC_COLUMNS:\n",
    "    _max = np.max(df_train[nc])\n",
    "    _min = np.min(df_train[nc])\n",
    "    df_train[nc] =  df_train[nc].parallel_apply(normalize, args=(_min,_max,))\n",
    "    df_test[nc] =  df_test[nc].parallel_apply(normalize, args=(_min,_max,))\n",
    "\n",
    "# ---------------------\n",
    "# Generate anomalies from test set\n",
    "# Count : 25% of the test set \n",
    "# ---------------------\n",
    "for nc in NUMERIC_COLUMNS:\n",
    "    numeric_col_stats[nc] = {}\n",
    "    numeric_col_stats[nc]['median'] = np.median(df_train[nc])\n",
    "    \n",
    "create_coocc_dict(df_train) \n",
    "\n",
    "df_1 = df_test.sample(frac=0.25)\n",
    "df_1 = df_1.reset_index(drop=True)\n",
    "df_3 = df_1.parallel_apply(\n",
    "    perturb_row,\n",
    "    axis=1\n",
    ")\n",
    "# Save anomalies \n",
    "df_anomalies = df_3.copy()\n",
    "df_anomalies.to_csv(os.path.join(save_dir,'gen_anomalies.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
